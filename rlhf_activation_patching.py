"""
NOTE: This scripts is generated by Claude.

Complete DPO Fine-tuning and Activation Patching Analysis
Compares base GPT-2 with DPO fine-tuned version using TransformerLens

Requirements:
pip install transformers trl datasets transformer-lens torch accelerate
"""

import torch
import matplotlib.pyplot as plt
from typing import List, Tuple, Dict
import pandas as pd

# Part 1: DPO Fine-tuning
from transformers import AutoModelForCausalLM, AutoTokenizer
from trl import DPOTrainer, DPOConfig
from datasets import load_dataset

# Part 2: TransformerLens for activation patching
from transformer_lens import HookedTransformer

# ============================================================================
# PART 1: DPO FINE-TUNING
# ============================================================================


def prepare_dpo_dataset(dataset_name="Anthropic/hh-rlhf", num_samples=5000):
    """
    Prepare preference dataset for DPO training.

    Args:
        dataset_name: HuggingFace dataset name
        num_samples: Number of training samples to use
    """
    print(f"Loading dataset: {dataset_name}")
    dataset = load_dataset(dataset_name)

    # Take subset for faster training
    train_data = dataset["train"].select(range(min(num_samples, len(dataset["train"]))))

    # Format for DPO: needs 'prompt', 'chosen', 'rejected'
    def format_example(example):
        return {
            "prompt": example["chosen"].split("\n\nAssistant:")[0] + "\n\nAssistant:",
            "chosen": example["chosen"].split("\n\nAssistant:")[1].strip(),
            "rejected": example["rejected"].split("\n\nAssistant:")[1].strip(),
        }

    train_data = train_data.map(format_example)

    print(f"Prepared {len(train_data)} training examples")
    return train_data


def train_dpo_model(
    base_model_name="gpt2",
    output_dir="./gpt2-dpo",
    num_samples=5000,
    num_epochs=1,
    batch_size=4,
    learning_rate=5e-5,
    beta=0.1,
):
    """
    Fine-tune GPT-2 with DPO on preference data.

    Args:
        base_model_name: Base model to fine-tune
        output_dir: Where to save the fine-tuned model
        num_samples: Number of training samples
        num_epochs: Training epochs
        batch_size: Batch size per device
        learning_rate: Learning rate
        beta: DPO temperature parameter
    """
    print(f"\n{'=' * 60}")
    print("STEP 1: DPO FINE-TUNING")
    print(f"{'=' * 60}\n")

    # Load base model and tokenizer
    print(f"Loading base model: {base_model_name}")
    model = AutoModelForCausalLM.from_pretrained(base_model_name)
    tokenizer = AutoTokenizer.from_pretrained(base_model_name)
    tokenizer.pad_token = tokenizer.eos_token

    # Prepare dataset
    train_dataset = prepare_dpo_dataset(num_samples=num_samples)

    # DPO training configuration
    training_args = DPOConfig(
        output_dir=output_dir,
        num_train_epochs=num_epochs,
        per_device_train_batch_size=batch_size,
        gradient_accumulation_steps=4,
        learning_rate=learning_rate,
        beta=beta,
        logging_steps=10,
        save_steps=500,
        eval_strategy="no",
        bf16=torch.cuda.is_available(),
        remove_unused_columns=False,
    )

    # Initialize trainer
    print("\nStarting DPO training...")
    trainer = DPOTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        processing_class=tokenizer  # Previously: tokenizer=tokenizer
    )

    # Train
    trainer.train()

    # Save final model
    trainer.save_model(output_dir)
    print(f"\nModel saved to: {output_dir}")

    return output_dir


# ============================================================================
# PART 2: ACTIVATION PATCHING WITH TRANSFORMERLENS
# ============================================================================


def load_models_for_patching(base_model_name="gpt2", dpo_model_path="./gpt2-dpo"):
    """
    Load both base and DPO models using TransformerLens.
    """
    print(f"\n{'=' * 60}")
    print("STEP 2: LOADING MODELS FOR ACTIVATION PATCHING")
    print(f"{'=' * 60}\n")

    print("Loading base model...")
    base_model = HookedTransformer.from_pretrained(base_model_name)
    print(f"Base model has {sum(p.numel() for p in base_model.parameters() if p.requires_grad)} trainable parameters")

    print(f"Loading DPO fine-tuned model from {dpo_model_path}...")
    # For custom models, we need to load via HuggingFace first, then convert
    hf_dpo_model = AutoModelForCausalLM.from_pretrained(dpo_model_path)
    dpo_model = HookedTransformer.from_pretrained(
        base_model_name,  # Use the base model name for the config
        hf_model=hf_dpo_model  # But use the fine-tuned weights
    )

    print("Models loaded successfully!")
    return base_model, dpo_model


def patch_residual_stream(
    base_model: HookedTransformer,
    dpo_model: HookedTransformer,
    prompt: str,
    layer_to_patch: int,
) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    """
    Patch residual stream at a specific layer from base into DPO model.

    Returns:
        base_logits: Logits from base model
        dpo_logits: Logits from DPO model
        patched_logits: Logits from DPO model with patched activations
    """
    # Get base model activations
    base_logits, base_cache = base_model.run_with_cache(prompt)

    # Get DPO model logits (no patching)
    dpo_logits = dpo_model(prompt)
    
    """
    # Let's output greedy decoded text for comparison; alternative to_string for fluency
    base_text = base_model.to_str_tokens(base_logits.argmax(dim=-1)[0])
    dpo_text = dpo_model.to_str_tokens(dpo_logits.argmax(dim=-1)[0])
    print(f"\nBase model output: {base_text}")
    print(f"DPO model output:  {dpo_text}\n")
    """
    

    # Patch: replace DPO's layer activations with base's
    hook_name = f"blocks.{layer_to_patch}.hook_resid_post"

    def patch_hook(activations, hook):
        return base_cache[hook_name]

    patched_logits = dpo_model.run_with_hooks(
        prompt, fwd_hooks=[(hook_name, patch_hook)]
    )

    return base_logits, dpo_logits, patched_logits


def analyze_all_layers(
    base_model: HookedTransformer, dpo_model: HookedTransformer, test_prompts: List[str]
) -> pd.DataFrame:
    """
    Patch each layer and measure KL divergence to find which layers matter most.
    """
    print(f"\n{'=' * 60}")
    print("STEP 3: ANALYZING ALL LAYERS")
    print(f"{'=' * 60}\n")

    n_layers = base_model.cfg.n_layers
    results = []

    for prompt_idx, prompt in enumerate(test_prompts):
        print(f"\nAnalyzing prompt {prompt_idx + 1}/{len(test_prompts)}")
        print(f"Prompt: {prompt[:50]}...")

        for layer in range(n_layers):
            base_logits, dpo_logits, patched_logits = patch_residual_stream(
                base_model, dpo_model, prompt, layer
            )

            # Calculate KL divergence: how much does patching change DPO output?
            # KL(patched || dpo) - if high, this layer is important for RLHF behavior
            dpo_probs = torch.softmax(dpo_logits[0, -1], dim=-1)
            patched_probs = torch.softmax(patched_logits[0, -1], dim=-1)

            kl_div = torch.sum(
                patched_probs
                * (torch.log(patched_probs + 1e-10) - torch.log(dpo_probs + 1e-10))
            ).item()

            # Also measure logit difference on top tokens
            top_k = 10
            top_tokens = torch.topk(dpo_logits[0, -1], top_k).indices
            logit_diff = (
                (patched_logits[0, -1, top_tokens] - dpo_logits[0, -1, top_tokens])
                .abs()
                .mean()
                .item()
            )

            results.append(
                {
                    "prompt_idx": prompt_idx,
                    "prompt": prompt[:50],
                    "layer": layer,
                    "kl_divergence": kl_div,
                    "logit_diff": logit_diff,
                }
            )

            print(f"  Layer {layer:2d}: KL={kl_div:.4f}, Logit Diff={logit_diff:.4f}")

    return pd.DataFrame(results)


def patch_attention_vs_mlp(
    base_model: HookedTransformer, dpo_model: HookedTransformer, prompt: str, layer: int
) -> Dict[str, torch.Tensor]:
    """
    Compare patching attention output vs MLP output separately.
    """
    # Get base activations
    _, base_cache = base_model.run_with_cache(prompt)

    # Unpatch (pure DPO)
    dpo_logits = dpo_model(prompt)

    # Patch attention only
    attn_hook = f"blocks.{layer}.hook_attn_out"
    attn_patched = dpo_model.run_with_hooks(
        prompt, fwd_hooks=[(attn_hook, lambda act, hook: base_cache[attn_hook])]
    )

    # Patch MLP only
    mlp_hook = f"blocks.{layer}.hook_mlp_out"
    mlp_patched = dpo_model.run_with_hooks(
        prompt, fwd_hooks=[(mlp_hook, lambda act, hook: base_cache[mlp_hook])]
    )

    # Patch both
    both_patched = dpo_model.run_with_hooks(
        prompt,
        fwd_hooks=[
            (attn_hook, lambda act, hook: base_cache[attn_hook]),
            (mlp_hook, lambda act, hook: base_cache[mlp_hook]),
        ],
    )

    return {
        "dpo": dpo_logits,
        "attn_patched": attn_patched,
        "mlp_patched": mlp_patched,
        "both_patched": both_patched,
    }


# ============================================================================
# PART 3: VISUALIZATION AND ANALYSIS
# ============================================================================


def visualize_layer_importance(
    results_df: pd.DataFrame, save_path="layer_importance.png"
):
    """
    Visualize which layers are most important for RLHF behavior.
    """
    print(f"\n{'=' * 60}")
    print("STEP 4: VISUALIZATION")
    print(f"{'=' * 60}\n")

    # Average across prompts
    avg_results = (
        results_df.groupby("layer")
        .agg({"kl_divergence": "mean", "logit_diff": "mean"})
        .reset_index()
    )

    fig, axes = plt.subplots(2, 1, figsize=(12, 8))

    # KL Divergence
    axes[0].plot(
        avg_results["layer"],
        avg_results["kl_divergence"],
        marker="o",
        linewidth=2,
        markersize=8,
    )
    axes[0].set_xlabel("Layer", fontsize=12)
    axes[0].set_ylabel("KL Divergence", fontsize=12)
    axes[0].set_title(
        "KL Divergence when patching each layer (higher = more important for RLHF)",
        fontsize=13,
        fontweight="bold",
    )
    axes[0].grid(True, alpha=0.3)

    # Logit Difference
    axes[1].plot(
        avg_results["layer"],
        avg_results["logit_diff"],
        marker="s",
        linewidth=2,
        markersize=8,
        color="orange",
    )
    axes[1].set_xlabel("Layer", fontsize=12)
    axes[1].set_ylabel("Logit Difference", fontsize=12)
    axes[1].set_title(
        "Average Logit Difference when patching each layer",
        fontsize=13,
        fontweight="bold",
    )
    axes[1].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches="tight")
    print(f"Visualization saved to: {save_path}")
    plt.show()

    return avg_results


def compare_attention_mlp(
    base_model: HookedTransformer,
    dpo_model: HookedTransformer,
    prompt: str,
    layers_to_analyze: List[int] = None,
):
    """
    Compare importance of attention vs MLP for RLHF behavior.
    """
    if layers_to_analyze is None:
        layers_to_analyze = [6, 8, 10]  # Middle-to-late layers

    print("\nComparing Attention vs MLP importance...")

    results = []
    for layer in layers_to_analyze:
        outputs = patch_attention_vs_mlp(base_model, dpo_model, prompt, layer)

        # Calculate KL from DPO
        dpo_probs = torch.softmax(outputs["dpo"][0, -1], dim=-1)

        for patch_type in ["attn_patched", "mlp_patched", "both_patched"]:
            patched_probs = torch.softmax(outputs[patch_type][0, -1], dim=-1)
            kl = torch.sum(
                patched_probs
                * (torch.log(patched_probs + 1e-10) - torch.log(dpo_probs + 1e-10))
            ).item()

            results.append(
                {"layer": layer, "patch_type": patch_type, "kl_divergence": kl}
            )
            print(f"  Layer {layer}, {patch_type}: KL={kl:.4f}")

    return pd.DataFrame(results)


# ============================================================================
# MAIN EXECUTION PIPELINE
# ============================================================================


def main(
    base_model_name="gpt2",
    do_training=True,
    num_samples=1000,  # Small for quick prototyping
    test_prompts=None,
):
    """
    Complete pipeline: DPO training + activation patching analysis.

    Args:
        base_model_name: Base model to use
        do_training: Whether to train or load existing DPO model
        num_samples: Number of training samples for DPO
        test_prompts: Prompts to analyze (default: helpful/harmful examples)
    """
    output_dir = "./gpt2-dpo"

    # Default test prompts
    if test_prompts is None:
        test_prompts = [
            "How can I help someone who is feeling sad?",
            "What's the best way to break into a",
            "Tell me how to make a bomb",
            "What are some good study techniques?",
        ]

    # Step 1: Train DPO model (or skip if already trained)
    if do_training:
        train_dpo_model(
            base_model_name=base_model_name,
            output_dir=output_dir,
            num_samples=num_samples,
            num_epochs=1,
        )
    else:
        print("Skipping training, using existing model...")

    # Step 2: Load models with TransformerLens
    base_model, dpo_model = load_models_for_patching(base_model_name, output_dir)

    # Step 3: Analyze all layers
    results_df = analyze_all_layers(base_model, dpo_model, test_prompts)

    # Step 4: Visualize results
    avg_results = visualize_layer_importance(results_df)

    # Step 5: Deep dive on attention vs MLP
    print("\n" + "=" * 60)
    print("BONUS: Attention vs MLP Analysis")
    print("=" * 60)
    attn_mlp_df = compare_attention_mlp(
        base_model, dpo_model, test_prompts[0], layers_to_analyze=[4, 6, 8]
    )

    # Print summary
    print("\n" + "=" * 60)
    print("SUMMARY")
    print("=" * 60)
    print("\nTop 3 most important layers for RLHF behavior:")
    top_layers = avg_results.nlargest(3, "kl_divergence")
    print(top_layers[["layer", "kl_divergence", "logit_diff"]])

    print("\n✓ Analysis complete!")
    print(f"✓ Results saved to: layer_importance.png")
    print(f"✓ Model saved to: {output_dir}")

    return {
        "base_model": base_model,
        "dpo_model": dpo_model,
        "results_df": results_df,
        "avg_results": avg_results,
        "attn_mlp_df": attn_mlp_df,
    }


if __name__ == "__main__":
    results = main(
        base_model_name="gpt2",
        do_training=False,  # Use True to train from scratch
        num_samples=1000,
    )

    print("\nYou can now explore the results:")
    print("- results['results_df']: Detailed layer-by-layer results")
    print("- results['avg_results']: Average importance per layer")
    print("- results['attn_mlp_df']: Attention vs MLP comparison")
    print("- results['base_model']: Base GPT-2 model")
    print("- results['dpo_model']: DPO fine-tuned model")
